{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankurs190/Agentic-2.0/blob/main/28_June_Day_15_Supervisor_multigent_(langgraph_class_7)_Ank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detailed Code"
      ],
      "metadata": {
        "id": "BwmaHM6D-P0r"
      },
      "id": "BwmaHM6D-P0r"
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install langchain_groq\n",
        "# %pip install langchain_community\n",
        "# %pip install langchain_experimental\n",
        "# %pip install langgraph\n",
        "GROQ_API_KEY=''\n",
        "TAVILY_API_KEY=''  #This key is required for authentication."
      ],
      "metadata": {
        "id": "cZXaJld7rRy1"
      },
      "id": "cZXaJld7rRy1",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "b8d8247c",
      "metadata": {
        "id": "b8d8247c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"]=GROQ_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "bf7fcee6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bf7fcee6",
        "outputId": "ecf7a683-66b1-4377-9741-2473260885b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<think>\\n\\n</think>\\n\\nThe capital of France is Paris.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq   # This imports the ChatGroq class from the langchain_groq module. ChatGroq is a wrapper that lets you use Groq-hosted LLMs with LangChain.\n",
        "\n",
        "llm=ChatGroq(model=\"deepseek-r1-distill-llama-70b\")   # This initializes a language model using Groq's infrastructure, specifically the \"deepseek-r1-distill-llama-70b\" model. This model is a 70-billion-parameter LLaMA-based language model optimized for performance.\n",
        "\n",
        "llm.invoke(\"What is the capital of France?\").content  # This sends a prompt to the model and receives a response. Extracts the actual text content from the model's response object.\n",
        "\n",
        "# print(llm.invoke(\"What is the capital of india tell me in detail?\").content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "2f3025e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2f3025e9",
        "outputId": "75dca1c6-f41e-4dfe-cded-0862017cba8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'Paris facts: the capital of France in history',\n",
              "  'url': 'https://home.adelphi.edu/~ca19535/page%204.html',\n",
              "  'content': 'page 4\\n\\n===============\\n\\nHomeSpainSydneySan FranciscoParisLas VegasMaui\\n\\n Paris, France\\n\\nImage 1\\n\\nParis facts: Paris, the capital of France\\n---------------------------------------------\\n\\nParis is the capital of France, the largest country of Europe with 550 000 km2 (65 millions inhabitants).\\n\\nParis has 2.234 million inhabitants end 2011. She is the core of Ile de France region (12 million people). [...] Before Paris, the capital of France was Lyon (under the Romans). Paris first became the capital of France in 508 under King Clovis. After centuries with no unique capital of France, Paris retrieved its status of capital of France under King Philippe Auguste, who reigned between 1180 and 1223. You can see remains of the Philippe August Paris walls in the passageway between the Louvre parking and Louvre Museum [...] Paris remained the capital of France until today, with one four year interruption. During German occupation (WW2 , 1940-1944), the capital of France was Vichy.\\n\\ngo to top\\n\\nReference:',\n",
              "  'score': 0.89166987},\n",
              " {'title': 'Paris - Wikipedia',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Paris',\n",
              "  'content': 'As the capital of France, Paris is the seat of France\\'s national government. For the executive, the two chief officers each have their own official residences, which also serve as their offices. The President of the French Republic resides at the Élysée Palace.( The Prime Minister\\'s seat is at the Hôtel Matignon.( Government ministries are located in various parts of the city, many near the Hôtel Matignon.( [...] Appearance\\n\\nmove to sidebar hide\\n\\nCoordinates: 48°51′24″N 2°21′8″E / 48.85667°N 2.35222°E / 48.85667; 2.35222_region:FR-75C)\\n\\nImage 4: This is a good article. Click here for more information.\\n\\nImage 5: Page semi-protected\\n\\nFrom Wikipedia, the free encyclopedia\\n\\nCapital and largest city of France\\n\\nThis article is about the capital city of France. For other uses, see Paris (disambiguation) \"Paris (disambiguation)\"). [...] Paris (French pronunciation:( \"Help:IPA/French\")ⓘ) is the capital and largest city of France. With an estimated population of 2,048,472 residents in January 2025( in an area of more than 105 km 2 (41 sq mi),( Paris is the fourth-most populous city in the European Union and the 30th most densely populated city in the world in 2022.( Since the 17th century, Paris has been one of the world\\'s major centres of finance, diplomacy, commerce, culture, fashion, and gastronomy. Because of its leading',\n",
              "  'score': 0.8815438},\n",
              " {'title': 'France | History, Maps, Flag, Population, Cities, Capital, & Facts',\n",
              "  'url': 'https://www.britannica.com/place/France',\n",
              "  'content': 'The capital and by far the most important city of France is Paris, one of the world’s preeminent cultural and commercial centres. A majestic city known as the ville lumière, or “city of light,” Paris has often been remade, most famously in the mid-19th century under the command of Georges-Eugène, Baron Haussman, who was committed to Napoleon III’s vision of a modern city free of the choleric swamps and congested alleys of old, with broad avenues and a regular plan. Paris is now a sprawling [...] Quick Facts\\n\\nFrance\\n\\nSee article: flag of France\\n\\nAudio File:\\nAnthem of France (see article)\\n\\nHead Of Government:\\n:   Prime minister: François Bayrou\\n\\n(Show more)\\n\\nCapital:\\n:   Paris\\n\\n(Show more)\\n\\nPopulation:\\n:   (2025 est.) 66,607,000\\n\\n(Show more)\\n\\nCurrency Exchange Rate:\\n:   1 USD equals 0.886 euro\\n\\n(Show more)\\n\\nHead Of State:\\n:   President: Emmanuel Macron\\n\\n(Show more)\\n\\nForm Of Government:\\n:   republic with two legislative houses (Parliament; Senate , National Assembly )\\n\\n(Show more) [...] Among France’s other major cities are Lyon, located along an ancient Rhône valley trade route linking the North Sea and the Mediterranean; Marseille, a multiethnic port on the Mediterranean founded as an entrepôt for Greek and Carthaginian traders in the 6th century bce; Nantes, an industrial centre and deepwater harbour along the Atlantic coast; and Bordeaux, located in southwestern France along the Garonne River.\\n\\nBritannica Chatbot logo\\n\\n# Britannica Chatbot',\n",
              "  'score': 0.8729662},\n",
              " {'title': 'France - Wikipedia',\n",
              "  'url': 'https://en.wikipedia.org/wiki/France',\n",
              "  'content': 'and its capital, largest city and main cultural and economic centre is Paris. [...] | France on the globe centred on Europe  Metropolitan France (European part of France) in Europe  France and its neighbors  Show France, its overseas territories and its exclusive economic zones  Location of France (blue or dark green) – in Europe (green & dark grey) – in the European Union (green) | |\\n| Capital and largest city | Paris 48°51′N 2°21′E\\ufeff / \\ufeff48.850°N 2.350°E\\ufeff / 48.850; 2.350_region:FR-75C) |\\n| Official language and national language | French | [...] France is a highly urbanised country, with its largest cities \"List of cities in France over 20,000 population (1999 census)\") (in terms of metropolitan area \"Functional area (France)\") population in 2021) being Paris (13,171,056 inh.), Lyon (2,308,818), Marseille (1,888,788), Lille (1,521,660), Toulouse (1,490,640), Bordeaux (1,393,764), Nantes (1,031,953), Strasbourg (864,993), Montpellier (823,120), and Rennes (771,320). (Note: since its 2020 revision of metropolitan area borders, INSEE',\n",
              "  'score': 0.79119295},\n",
              " {'title': 'Paris | Definition, Map, Population, Facts, & History - Britannica',\n",
              "  'url': 'https://www.britannica.com/place/Paris',\n",
              "  'content': 'Paris occupies a depression hollowed out by the Seine. The surrounding heights have elevations that vary from 430 feet (130 meters), at the butte of Montmartre in the north, to 85 feet (26 meters), in the Grenelle area in the southwest. The city is surrounded by great forests of beech and oak, called the “lungs of Paris,” as they help purify the air in the region.\\n\\n### Paris is the capital of what country?\\n\\nParis is the national capital of France.\\n\\n## News • [...] Paris, city and capital of France, situated in the north-central part of the country. People were living on the site of the present-day city, located along the Seine River some 233 miles (375 km) upstream from the river’s mouth on the English Channel (La Manche), by about 7600 bce. The modern city has spread from the island (the Île de la Cité) and far beyond both banks of the Seine.\\n\\nParis\\n\\nParis(more) [...] Paris; Eiffel Tower\\n\\nParis; Eiffel Tower\\nView of the Paris skyline from Montparnasse.\\n\\n(more)\\n\\n# Paris\\n\\nnational capital, France\\n\\nAsk the Chatbot a Question\\n\\nMore Actions\\n\\nPrint\\n\\nCite\\n\\nShare\\n\\nFeedback\\n\\nExternal Websites\\n\\nAlso known as: Lutetia\\n\\nWritten by\\n\\nBlake Ehrlich\\n\\nAuthor of Paris on the Seine; London on the Thames.\\n\\nBlake Ehrlich\\n,\\n\\nKimberly Daul',\n",
              "  'score': 0.78194773}]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults  #This imports the TavilySearchResults class from the community-contributed tools in LangChain. TavilySearchResults allows you to query the web using the Tavily API (a real-time search API).\n",
        "\n",
        "search_tool = TavilySearchResults(tavily_api_key=TAVILY_API_KEY) #This creates an instance of the TavilySearchResults tool. This key is required for authentication.\n",
        "\n",
        "search_tool.invoke(\"What is the capital of France?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "5437036d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5437036d",
        "outputId": "f8e5c52d-b787-4aa4-aef8-aee651ad09ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'20\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "my_code = \"\"\"\n",
        "x=10\n",
        "y=x+10\n",
        "print(y)\n",
        "\"\"\"\n",
        "\n",
        "from langchain_experimental.utilities import PythonREPL  # Useful for evaluating Python code snippets during runtime inside an agent or toolchain.\n",
        "repl=PythonREPL()  # This initializes the REPL (Read-Eval-Print Loop) object.\n",
        "repl.run(my_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "790086cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "collapsed": true,
        "id": "790086cd",
        "outputId": "e624d2b6-17f7-45de-ab22-88a149ad8a2f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'PythonREPL' object has no attribute 'invoke'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-72-2870411329.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrepl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_code\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# run vs invoke ; AttributeError: 'PythonREPL' object has no attribute 'invoke'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    989\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m                         \u001b[0;31m# this is the current error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{type(self).__name__!r} object has no attribute {item!r}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PythonREPL' object has no attribute 'invoke'"
          ]
        }
      ],
      "source": [
        "repl.invoke(my_code)  # run vs invoke ; AttributeError: 'PythonREPL' object has no attribute 'invoke'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5d7721e",
      "metadata": {
        "id": "c5d7721e"
      },
      "outputs": [],
      "source": [
        "# creating a LangChain-native tool using @tool\n",
        "\n",
        "from typing import Annotated  #imports Annotated, which allows you to attach metadata or descriptions to function parameters.\n",
        "from langchain_core.tools import tool  # Imports the @tool decorator from LangChain Core.This decorator converts a normal Python function into a LangChain-compatible Tool that agents or chains can use.\n",
        "\n",
        "@tool  # This marks the function below as a LangChain tool. The function becomes usable in an agent/toolchain and supports .invoke(...).\n",
        "def python_repl_tool(code: Annotated[str, \"The python code to execute to generate your chart.\"]): # code is a string input parameter, and the Annotated[...] part adds a description:\n",
        "      # A docstring that explains how to use the tool.\n",
        "    \"\"\"Use this to execute python code and do math. If you want to see the output of a value,\n",
        "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
        "\n",
        "    try:\n",
        "        result = repl.run(code)   # Tries to run the given code/ input using the repl.run() method (from PythonREPL).\n",
        "    except BaseException as e: #If any exception occurs during code execution, it is caught here.\n",
        "        return f\"Failed to execute. Error: {repr(e)}\"  # Returns a safe error message with the exact exception type and message.\n",
        "\n",
        "    result_str = f\"Successfully executed:\\n\\`\\`\\`python\\n{code}\\n\\`\\`\\`\\nStdout: {result}\"\n",
        "    return result_str\n",
        "\n",
        "python_repl_tool.invoke(\"x=10\\ny=x+10\\nprint(y)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e78c0cad",
      "metadata": {
        "id": "e78c0cad"
      },
      "source": [
        "- WE HAVE TWO SUB AGENT\n",
        "  - RESEARCHER- internet\n",
        "  - CODER- executing the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c12eecfe",
      "metadata": {
        "id": "c12eecfe"
      },
      "outputs": [],
      "source": [
        "members=[\"researcher\",\"coder\"]\n",
        "options = members+[\"FINISH\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e24282f",
      "metadata": {
        "id": "9e24282f"
      },
      "outputs": [],
      "source": [
        "from typing import Literal  # Literal is used to indicate that a variable or parameter can only take specific, fixed values. This is useful for type checking, validation, and LLM tool schemas\n",
        "from typing_extensions import TypedDict  # TypedDict allows you to define a dictionary with a known structure (i.e., typed keys and values).\n",
        "\n",
        "# Literal\n",
        "\n",
        "def get_shape(s: Literal[\"circle\", \"square\", \"triangle\"]) -> str:  # return type str\n",
        "    return f\"Shape selected: {s}\"\n",
        "\n",
        "\n",
        "# TypedDict\n",
        "class Person(TypedDict):\n",
        "    name: str\n",
        "    age: int\n",
        "\n",
        "p= Person({\"name\": \"Alice\", \"age\": 30}) # Here, p must have both \"name\" and \"age\" keys with correct types.\n",
        "# p: Person = {\"name\": \"Alice\", \"age\": 30}\n",
        "p"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8049ed75",
      "metadata": {
        "id": "8049ed75"
      },
      "source": [
        "- There is no routing logic\n",
        "- it is simply going to return the next candidate(next_agent)\n",
        "- this next is containig the next candidate name\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "72d958d1",
      "metadata": {
        "id": "72d958d1"
      },
      "outputs": [],
      "source": [
        "class Router(TypedDict):   # This defines a typed dictionary, which is like a regular Python dict, but with a fixed key-value structure enforced at type-checking time\n",
        "    next: Literal['researcher', 'coder', 'FINISH']\n",
        "# defines a TypedDict named Router with a single key next, whose value must be one of three specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "2cd295dc",
      "metadata": {
        "id": "2cd295dc"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import MessagesState,StateGraph,START, END\n",
        "\n",
        "# builder.add_edge(START, \"research\")\n",
        "# builder.add_edge(\"coder\", END)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae1f83b3",
      "metadata": {
        "id": "ae1f83b3"
      },
      "source": [
        "#### this is a messagesstate which we are loading from the langgraph(inbuilt message state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "c8158d0b",
      "metadata": {
        "id": "c8158d0b"
      },
      "outputs": [],
      "source": [
        "# class MessagesState(TypedDict):\n",
        "#     messages: Annotated[list[AnyMessage], add_messages]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "a5227179",
      "metadata": {
        "id": "a5227179"
      },
      "outputs": [],
      "source": [
        "# class State(MessagesState):\n",
        "#     next:str"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a84759",
      "metadata": {
        "id": "b2a84759"
      },
      "source": [
        "##### this is how my state will be looking like"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ankur example\n",
        "# MessagesState is a predefined state structure in LangGraph that keeps track of a list of chat messages between you (the user) and the assistant (the AI).\n",
        "# Under the hood, it’s just a TypedDict like this:\n",
        "\n",
        "# class MessagesState(TypedDict):\n",
        "#     messages: list\n",
        "\n",
        "# state = {\n",
        "#   \"messages\": [\n",
        "#     {\"role\": \"user\", \"content\": \"What is 2 + 2?\"},\n",
        "#     {\"role\": \"assistant\", \"content\": \"It's 4!\"}\n",
        "#   ]\n",
        "# }\n",
        "\n",
        "# Because LangGraph nodes (like agents or tools) can read and add to this list, which lets:\n",
        "# Agents remember the conversation\n",
        "# Messages flow from node to node\n",
        "# You build multi-step chat workflows\n",
        "\n"
      ],
      "metadata": {
        "id": "7wRCzgT-2kGf"
      },
      "id": "7wRCzgT-2kGf",
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "a154231b",
      "metadata": {
        "id": "a154231b"
      },
      "outputs": [],
      "source": [
        "class State(MessagesState):\n",
        "    next:str\n",
        "state={\"messages\": [\"hi\"], \"next\": \"research_agent\"}\n",
        "\n",
        "#LangGraph needs to track what’s been said and who should go next.\n",
        "\n",
        "# This dictionary matches the State type because:\n",
        "# ✅ It has \"messages\" — a list of things people have said.\n",
        "# ✅ It has \"next\" — a string that tells us the next step (in this case, \"research_agent\").\n",
        "\n",
        "# # With Inheritance:\n",
        "\n",
        "# class State(MessagesState):\n",
        "#     next: str\n",
        "# Now you track messages and the next step in your flow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "791ffeba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "791ffeba",
        "outputId": "8c89095b-fee4-463c-bd8a-2d2036857e19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"\n",
            "You are a supervisor, tasked with managing a conversation between the following workers: ['researcher', 'coder']. \n",
            "Given the following user request, respond with the worker to act next. \n",
            "Each worker will perform a task and respond with their results and status. \n",
            "When finished, respond with FINISH.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "system_prompt = f\"\"\"\"\n",
        "You are a supervisor, tasked with managing a conversation between the following workers: {members}.\n",
        "Given the following user request, respond with the worker to act next.\n",
        "Each worker will perform a task and respond with their results and status.\n",
        "When finished, respond with FINISH.\n",
        "\"\"\"\n",
        "print(system_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "1fc1b413",
      "metadata": {
        "id": "1fc1b413"
      },
      "outputs": [],
      "source": [
        "messages = [{\"role\": \"system\", \"content\": system_prompt},] + state[\"messages\"]\n",
        "\n",
        "# Imagine you’re writing a story, and the first page is always instructions from the teacher (the “system”).Then the rest of the story is written by your friends (the “messages” from the conversation).\n",
        "# So this line is saying:  “Start with the system’s message (the rules), and then add the rest of the conversation after that.”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "5d7689a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d7689a2",
        "outputId": "5aec6987-9d32-4aed-8646-fd6d194142e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': '\"\\nYou are a supervisor, tasked with managing a conversation between the following workers: [\\'researcher\\', \\'coder\\']. \\nGiven the following user request, respond with the worker to act next. \\nEach worker will perform a task and respond with their results and status. \\nWhen finished, respond with FINISH.\\n'},\n",
              " 'hi']"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0abc374e",
      "metadata": {
        "id": "0abc374e"
      },
      "source": [
        "[{'role': 'system',\n",
        "  'content': '\"\\nYou are a supervisor, tasked with managing a conversation between the following workers: [\\'researcher\\', \\'coder\\']. \\nGiven the following user request, respond with the worker to act next. \\nEach worker will perform a task and respond with their results and status. \\nWhen finished, respond with FINISH.\\n**Strict Guidelines:**\\nif there is any common messages like hi, hello, how are you, greetings etc then,respond with FINISH.\\n'},\n",
        " 'hi']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Router(TypedDict):\n",
        "   next: Literal['researcher', 'coder', 'FINISH'] # This defines a structured format (a schema) for what kind of answer you're expecting from the LLM:\n",
        "   # It's a dictionary with one key: \"next\"The value for \"next\" must be:\"researcher\", or \"coder\", or \"FINISH\"\n",
        "   # \"Don't just reply with a paragraph. I want you to respond in a specific format, like: { \"next\": \"coder\" }\"\n",
        "\n",
        "# So your program can trust the output and use it directly — for example, to decide:\n",
        "# which agent to run next in LangGraph, when to end the loop, or how to update state.\n",
        "\n",
        "llm_with_structure_output=llm.with_structured_output(Router)  # “Here’s the Router format. You must follow it when replying.”\n",
        "\n",
        "llm_with_structure_output.invoke(messages) # output: {\"next\": \"researcher\"}  # structured, validated\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyA7BJtJ33JI",
        "outputId": "b1aa9ed1-71a3-41fc-ef3f-ba1a7afd7052"
      },
      "id": "vyA7BJtJ33JI",
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'next': 'researcher'}"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0550e25",
      "metadata": {
        "id": "e0550e25"
      },
      "source": [
        "#### you can try out with this prompt also"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "771b69cf",
      "metadata": {
        "id": "771b69cf"
      },
      "outputs": [],
      "source": [
        "# system_prompt = f\"\"\"\n",
        "# You are a supervisor managing a task delegation system with the following workers: {members}.\n",
        "\n",
        "# Your job is to decide which worker should act next based on the user’s input.\n",
        "\n",
        "# Guidelines:\n",
        "# - Carefully read the user’s message.\n",
        "# - If the message clearly requires a specific action (e.g., search, compute, rewrite), assign it to the appropriate worker.\n",
        "# - If the message is general, conversational, or does **not** require any specific action, immediately respond with `FINISH`.\n",
        "# - Do **not** invent tasks or assign actions unless the message clearly demands it.\n",
        "\n",
        "# Each worker will return results after completing their task.\n",
        "# Once all necessary tasks are completed, end the flow by responding with `FINISH`.\n",
        "\n",
        "# Be strict — if the message is casual, rhetorical, or lacks a clear task, reply with `FINISH`.\n",
        "# \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5679fab",
      "metadata": {
        "id": "f5679fab"
      },
      "source": [
        "#### This is my all three agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "4066541b",
      "metadata": {
        "id": "4066541b"
      },
      "outputs": [],
      "source": [
        "from langgraph.types import Command"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This supervisor_agent acts like a router node:\n",
        "  - Reads messages,\n",
        "  - Asks the LLM what to do next,\n",
        "  - Routes execution to the correct agent (researcher, coder, or END),\n",
        "  - Updates state accordingly."
      ],
      "metadata": {
        "id": "ZR-zdU090qwT"
      },
      "id": "ZR-zdU090qwT"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "a9dd8853",
      "metadata": {
        "id": "a9dd8853"
      },
      "outputs": [],
      "source": [
        "def supervisor_agent(state:State)->Command[Literal['researcher', 'coder', '__end__']]:\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt},] + state[\"messages\"]  # Prepends a system message (your prompt guide) to the user/assistant messages in the current state. This full message list is what the LLM will use to reason about the next step.\n",
        "\n",
        "    llm_with_structure_output=llm.with_structured_output(Router) # Tells the LLM to conform its output to a strict schema defined by Router:Ensures the model returns a dictionary like {'next': 'researcher'}.\n",
        "\n",
        "    response=llm_with_structure_output.invoke(messages) # Sends the full message list to the LLM. Receives structured output like: {\"next\": \"researcher\"}\n",
        "\n",
        "    #this is my response {'next': 'researcher'}\n",
        "\n",
        "    #this is my next worker agent\n",
        "    goto=response[\"next\"]  # Extract Routing Target: Extracts the \"next\" step name (researcher, coder, or FINISH).\n",
        "\n",
        "    print(\"**********BELOW IS MY GOTO***************\")\n",
        "\n",
        "    print(goto)  #Prints where the execution is about to go (for debugging/traceability).\n",
        "\n",
        "    if goto == \"FINISH\":  #If the LLM says \"FINISH\", replace it with END (LangGraph constant that marks completion).\n",
        "        goto=END\n",
        "\n",
        "    # class State(MessagesState):\n",
        "    #   next:str\n",
        "    # output of the state: state={\"messages\": [\"hi\"], \"next\": \"researcher\"}\n",
        "\n",
        "    return Command(goto=goto, update={\"next\":goto})  # Return Next Step Command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "a525e86e",
      "metadata": {
        "id": "a525e86e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cdb3b21",
      "metadata": {
        "id": "0cdb3b21"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "5bbd8ed7",
      "metadata": {
        "id": "5bbd8ed7"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "\n",
        "# Takes in the current state (which contains the conversation so far), Returns a Command that tells LangGraph: What to update in the state, Where to go next (in this case: \"supervisor\").\n",
        "def research_agent(state: State) -> Command[Literal[\"supervisor\"]]:\n",
        "\n",
        "    research_agent = create_react_agent(llm, tools=[search_tool], prompt=\"You are a researcher. DO NOT do any math.\")  # Create a ReAct Agen using LangChain's built-in helper create_react_agent\n",
        "\n",
        "    result=research_agent.invoke(state)   # state contains the message history — so the agent knows what it’s responding to.\n",
        "\n",
        "    return Command(\n",
        "        update={\n",
        "            \"messages\": [\n",
        "                HumanMessage(content=result[\"messages\"][-1].content, name=\"researcher\")  # \"Why are we returning a HumanMessage in the update if it's the AI (researcher agent) replying?\"\n",
        "                # result[\"messages\"][-1] is the last message from the research_agent (which is an AI).But instead of wrapping it as an AIMessage, you wrap it as a HumanMessage with name=\"researcher\".\n",
        "            ]\n",
        "        },\n",
        "        goto=\"supervisor\",\n",
        "    )  # Hey, before you move to the next node (supervisor), please update the state’s messages with this new message from the researcher.”\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # state update process\n",
        "\n",
        "# # Suppose the current state looks like this:\n",
        "# current_state= {\n",
        "#   \"messages\": [\n",
        "#     {\"role\": \"user\", \"content\": \"Who discovered gravity?\"}\n",
        "#   ],\n",
        "#   \"next\": \"research_agent\"\n",
        "# }\n",
        "\n",
        "# # Then update={\"messages\": [new_message]} replaces the \"messages\" list with the new one you provide.\n",
        "# new_state= {\n",
        "#   \"messages\": [\n",
        "#     {\"role\": \"user\", \"content\": \"Who discovered gravity?\"},\n",
        "#     {\"role\": \"human\", \"name\": \"researcher\", \"content\": \"Isaac Newton discovered gravity.\"}\n",
        "#   ],\n",
        "#   \"next\": \"research_agent\"\n",
        "# }\n",
        ""
      ],
      "metadata": {
        "id": "BJiuPg3f9Alz"
      },
      "id": "BJiuPg3f9Alz",
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "3b99ff85",
      "metadata": {
        "id": "3b99ff85"
      },
      "outputs": [],
      "source": [
        "def coder_agent(state:State)->Command[Literal['supervisor']]:\n",
        "    code_agent=create_react_agent(llm,tools=[python_repl_tool], prompt=\"You are a coder. DO NOT do any research.\")\n",
        "    result=code_agent.invoke(state)\n",
        "\n",
        "    return Command(\n",
        "        update={\n",
        "            \"messages\": [\n",
        "                HumanMessage(content=result[\"messages\"][-1].content, name=\"coder\")\n",
        "            ]\n",
        "        },\n",
        "        goto=\"supervisor\",\n",
        "    )\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beb7d161",
      "metadata": {
        "id": "beb7d161"
      },
      "source": [
        "#### this is my orchestration flow with langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "626bb385",
      "metadata": {
        "id": "626bb385"
      },
      "outputs": [],
      "source": [
        "graph=StateGraph(State)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "5ff8c103",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ff8c103",
        "outputId": "7fd1aeba-8f86-4253-e9a2-93f26527b140"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7fc122012390>"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "graph.add_node(\"supervisor\", supervisor_agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "5e33a98e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e33a98e",
        "outputId": "57c2bec1-aef7-41ad-e3bf-456a9caa00ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7fc122012390>"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "graph.add_node(\"researcher\", research_agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "559d4010",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "559d4010",
        "outputId": "f266cc4e-d3d3-4b89-8ce9-ecadb6fbca80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7fc122012390>"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "graph.add_node(\"coder\", coder_agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "7e2aad9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e2aad9a",
        "outputId": "e27aff45-10c8-42f5-f047-60b7c4c573ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7fc122012390>"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "graph.add_edge(START, \"supervisor\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "4ba88340",
      "metadata": {
        "id": "4ba88340"
      },
      "outputs": [],
      "source": [
        "app=graph.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "833b0f2d",
      "metadata": {
        "id": "833b0f2d"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display,Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "9c848f24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "9c848f24",
        "outputId": "9ca2b822-3738-4174-cde2-c30a9e40a8ef"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(Image(app.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "70212fda",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "70212fda",
        "outputId": "289484e8-4bc8-4e9f-9d99-46320ebc58f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********BELOW IS MY GOTO***************\n",
            "coder\n",
            "((), {'supervisor': {'next': 'coder'}})\n",
            "**********BELOW IS MY STATE***************\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'python_repl_tool' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-106-2486392257.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"What's the square root of 42?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubgraphs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"**********BELOW IS MY STATE***************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2531\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2533\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2534\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2535\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-95-2396260197.py\u001b[0m in \u001b[0;36mcoder_agent\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcoder_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'supervisor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcode_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_react_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpython_repl_tool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"You are a coder. DO NOT do any research.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcode_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     return Command(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'python_repl_tool' is not defined"
          ]
        }
      ],
      "source": [
        "for s in app.stream({\"messages\": [(\"user\", \"What's the square root of 42?\")]}, subgraphs=True):\n",
        "    print(s)\n",
        "    print(\"**********BELOW IS MY STATE***************\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01ff96b1",
      "metadata": {
        "id": "01ff96b1",
        "outputId": "f84b050d-199b-4ae7-f216-9759c1fe068c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********BELOW IS MY GOTO***************\n",
            "coder\n",
            "**********BELOW IS MY GOTO***************\n",
            "researcher\n",
            "**********BELOW IS MY GOTO***************\n",
            "coder\n"
          ]
        },
        {
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': ''}}",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[129], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result\u001b[38;5;241m=\u001b[39m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat is an efficent python code to get prime number?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\site-packages\\langgraph\\pregel\\__init__.py:2719\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[0m\n\u001b[0;32m   2716\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2717\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 2719\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   2720\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2721\u001b[0m     config,\n\u001b[0;32m   2722\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[0;32m   2723\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   2724\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   2725\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   2726\u001b[0m     checkpoint_during\u001b[38;5;241m=\u001b[39mcheckpoint_during,\n\u001b[0;32m   2727\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2729\u001b[0m ):\n\u001b[0;32m   2730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2731\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2732\u001b[0m             \u001b[38;5;28misinstance\u001b[39m(chunk, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m   2733\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m (ints \u001b[38;5;241m:=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mget(INTERRUPT)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2734\u001b[0m         ):\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\site-packages\\langgraph\\pregel\\__init__.py:2436\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[0;32m   2434\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[0;32m   2435\u001b[0m             loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 2436\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   2437\u001b[0m             [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[0;32m   2438\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   2439\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   2440\u001b[0m             schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[0;32m   2441\u001b[0m         ):\n\u001b[0;32m   2442\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   2443\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[0;32m   2444\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\site-packages\\langgraph\\pregel\\runner.py:252\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 252\u001b[0m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tb \u001b[38;5;241m:=\u001b[39m exc\u001b[38;5;241m.\u001b[39m__traceback__:\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\site-packages\\langgraph\\pregel\\runner.py:509\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[1;34m(futs, timeout_exc_cls, panic)\u001b[0m\n\u001b[0;32m    507\u001b[0m                 interrupts\u001b[38;5;241m.\u001b[39mappend(exc)\n\u001b[0;32m    508\u001b[0m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[1;32m--> 509\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\site-packages\\langgraph\\pregel\\executor.py:80\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 80\u001b[0m     \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\site-packages\\langgraph\\utils\\runnable.py:623\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m--> 623\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\site-packages\\langgraph\\utils\\runnable.py:377\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 377\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "Cell \u001b[1;32mIn[78], line 7\u001b[0m, in \u001b[0;36msupervisor_agent\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      3\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt},] \u001b[38;5;241m+\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m llm_with_structure_output\u001b[38;5;241m=\u001b[39mllm\u001b[38;5;241m.\u001b[39mwith_structured_output(Router)\n\u001b[1;32m----> 7\u001b[0m response\u001b[38;5;241m=\u001b[39m\u001b[43mllm_with_structure_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#this is my next worker agent\u001b[39;00m\n\u001b[0;32m     10\u001b[0m goto\u001b[38;5;241m=\u001b[39mresponse[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\site-packages\\langchain_core\\runnables\\base.py:3045\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3043\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m   3044\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3045\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3046\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3047\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config)\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\site-packages\\langchain_core\\runnables\\base.py:5431\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5424\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   5425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5426\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5429\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   5432\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5433\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5434\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5435\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:372\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    368\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    369\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 372\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    373\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    374\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    375\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    376\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    377\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    378\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    379\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    380\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    381\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    382\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:957\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    955\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    956\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:776\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    774\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    775\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 776\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    777\u001b[0m                 m,\n\u001b[0;32m    778\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    779\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    780\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    781\u001b[0m             )\n\u001b[0;32m    782\u001b[0m         )\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1022\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1022\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m   1023\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1024\u001b[0m     )\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1026\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\site-packages\\langchain_groq\\chat_models.py:498\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    494\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    497\u001b[0m }\n\u001b[1;32m--> 498\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\site-packages\\groq\\resources\\chat\\completions.py:368\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    231\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexclude_domains\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minclude_domains\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearch_settings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\site-packages\\groq\\_base_client.py:1225\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1212\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1213\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1220\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1222\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1223\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1224\u001b[0m     )\n\u001b[1;32m-> 1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[1;32mc:\\Complete_Content2\\Agentic-2.0\\env\\lib\\site-packages\\groq\\_base_client.py:1034\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1031\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1033\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1034\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': ''}}"
          ]
        }
      ],
      "source": [
        "result=app.invoke({\"messages\": [(\"user\", \"what is an efficent python code to get prime number?\")]}, subgraphs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "6422f157",
      "metadata": {
        "id": "6422f157"
      },
      "outputs": [],
      "source": [
        "result=app.ainvoke({\"messages\": [(\"user\", \"what is an efficent python code to get prime number?\")]}, subgraphs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "2251b2ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2251b2ac",
        "outputId": "795ed8da-abeb-4c2f-de17-4f351b8c887d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<coroutine object Pregel.ainvoke at 0x7fc12213d840>"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarized code"
      ],
      "metadata": {
        "id": "2oXuLYjP-Ucp"
      },
      "id": "2oXuLYjP-Ucp"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ls1GPKv0-T7W"
      },
      "id": "Ls1GPKv0-T7W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZLdwY1Y-H2h"
      },
      "id": "KZLdwY1Y-H2h",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}